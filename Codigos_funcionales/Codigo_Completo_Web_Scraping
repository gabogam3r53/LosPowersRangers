import requests
from bs4 import BeautifulSoup
import csv
import pandas as pd
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import time
import numpy as np

#Constantes
#csv_file = 'data.csv'
link_jugadoras_precio = { 2024 : 'https://www.spotrac.com/wnba/rankings/player/_/year/2024/sort/contract_value',
                         2023 : 'https://www.spotrac.com/wnba/rankings/player/_/year/2023/sort/contract_value',
                         2022 :'https://www.spotrac.com/wnba/rankings/player/_/year/2022/sort/contract_value',
                         2021 : 'https://www.spotrac.com/wnba/rankings/player/_/year/2021/sort/contract_value',
                         2020 : 'https://www.spotrac.com/wnba/rankings/player/_/year/2020/sort/contract_value'}

link_jugadoras_stats = { 2024 : 'https://stats.wnba.com/players/traditional/?sort=PTS&dir=-1&Season=2024&SeasonType=Regular%20Season',
                         2023 : 'https://www.spotrac.com/wnba/rankings/player/_/year/2023/sort/contract_value',
                         2022 :'https://www.spotrac.com/wnba/rankings/player/_/year/2022/sort/contract_value',
                         2021 : 'https://www.spotrac.com/wnba/rankings/player/_/year/2021/sort/contract_value',
                         2020 : 'https://www.spotrac.com/wnba/rankings/player/_/year/2020/sort/contract_value'}

#Procedimientos iniciales
# Configurar el controlador de Selenium utilizando WebDriver Manager
#ervice = Service(ChromeDriverManager().install())
#driver = webdriver.Chrome(service=service)


#Funciones

def obtener_nombre_jugadoras(link):
    jugadoras = {'Nombre': []}
    r = requests.get(link) #Esto te da el codigo html de la pagina
    soup = BeautifulSoup(r.text, 'html.parser')
    links_nombres = soup.findAll('div', class_='link')
    for link_nombre in links_nombres:
        jugadoras['Nombre'].append(separar(link_nombre))
    df = pd.DataFrame(jugadoras)
    return df

def obtener_precio_jugadoras(link,jugadoras, numero_contrato):
    nueva_columna_contrato = 'Contrato {}'.format(numero_contrato)
    nueva_columna_equioi = 'Equipo {}'.format(numero_contrato)
    df = {'Nombre' : [] , 'Contrato' : [],'Fecha' :[]}
    r = requests.get(link) #Esto te da el codigo html de la pagina
    soup = BeautifulSoup(r.text, 'html.parser')
    links_nombres = soup.findAll('div', class_='link')
    links_montos = soup.findAll('span', class_= 'medium')
    links_fechas = soup.findAll('small',class_= 'mt-0 d-block')
    print(links_fechas[0].string)
    for link_nombre,link_monto,link_fecha in zip(links_nombres,links_montos,links_fechas):
        #if separar(link_nombre) != "A'ja Wilson" and separar(link_nombre) != 'Azurá Stevens' and separar(link_nombre) != 'Stephanie Soares':
        
        df['Nombre'].append(separar(link_nombre))
        df['Contrato'].append(int((separar(link_monto).strip()[1:]).replace(",","")))
        df['Fecha'].append(link_fecha.string.strip().split(',')[0])
    
    if len(df['Contrato']) < len(jugadoras['Nombre']):
        while len(df['Contrato']) != len(jugadoras['Nombre']):
            df['Contrato'].append(np.nan)
            df['Nombre'].append(np.nan)
   
    for nombre, valor,fecha in zip(df['Nombre'],df['Contrato'],df['Fecha']):
        if nombre in jugadoras['Nombre'].values:
            jugadoras.loc[jugadoras['Nombre'] == nombre, nueva_columna_contrato] = valor
            jugadoras.loc[jugadoras['Nombre'] == nombre, nueva_columna_equioi] = fecha

    return jugadoras

def separar(link):
    link = str(link)
    start_tag = False
    for i in range(len(link)):
        if link[i] == '<' and not start_tag:
            start_tag = True
        elif link[i] == '>' and start_tag:
            index_start_information = i + 1
        elif link[i] == '<' and start_tag:
            index_stop_information = i
            start_tag = False
    return link[index_start_information:index_stop_information]

def obtener_equipo_fecha_fichaje(nombre_jugadora):
    #Pagina a navegar
    url = 'https://www.365scores.com/es'
    driver.get(url)
    
    # Esperar a que el botón de búsqueda esté presente y hacer clic en él
    try:
        boton_buscar = WebDriverWait(driver, 10).until(
            EC.element_to_be_clickable((By.CSS_SELECTOR, '.site-header_search_button__3pJPq'))
        )
        boton_buscar.click()
        print("Botón de búsqueda clickeado.")
    except Exception as e:
        print("No se pudo hacer clic en el botón de búsqueda:", e)
        return
    
    # Esperar a que el campo de búsqueda esté presente, hacer clic y añade el nombre de la jugadora
    try:
        
        
        campo_busqueda = WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.CSS_SELECTOR, '.new-search-widget_input__aoNqC'))
        )
        
        campo_busqueda.click()
        campo_busqueda.send_keys(nombre_jugadora)
        time.sleep(2) #Se espera un poco para que cargue la jugadora a buscar
        campo_busqueda.send_keys(Keys.RETURN)  # Simular la tecla Enter
        print(f"Nombre de jugadora '{nombre_jugadora}' ingresado y búsqueda iniciada.")
    except Exception as e:
        print("No se pudo ingresar el nombre de la jugadora:", e)
        return
    try:   
        container = WebDriverWait(driver, 10).until(EC.visibility_of_element_located((By.CLASS_NAME, "new-search-widget_entity_item_intersection__L-eHP"))) 
        '''
        link_elemento = WebDriverWait(driver, 10).until(  #Le da click a la jugadora
            container.presence_of_element_located((By.CLASS_NAME, 'new-search-widget_entity_item_intersection__L-eHP'))
        )
        '''
        nombre_jugador_normalizado = nombre_jugadora.lower().replace(" ", "-")
        link = driver.find_element(By.XPATH, f".//a[contains(@class, 'new-search-widget_entity_item_intersection__L-eHP') and contains(@href, '{nombre_jugador_normalizado}')]")
        #container = driver.find_element(By.XPATH, ".//a[contains(@class, 'new-search-widget_entity_item_intersection__L-eHP') and contains(@href, '{nombre_jugadora}')]")
        #container = driver.find_element(By.CLASS_NAME, "new-search-widget_entity_item__Tthzc")
        #link_elemento = container.find_element(By.CLASS_NAME, 'new-search-widget_entity_item_intersection__L-eHP')
        #link_elemento.click()
        link.click()
    except Exception as e:
        print(f"No se pudo obtener el enlace o navegar al enlace de la jugadora '{nombre_jugadora}':", e)
        data = {
        'Nombre': [np.nan],
        'Fecha': [np.nan]
        }
    
        df = pd.DataFrame(data)
        return df
    
    #Se toma el contenedor que nos interesa y se guarda el html de este
    try:
        list_container = WebDriverWait(driver, 10).until(
        EC.visibility_of_element_located((By.CLASS_NAME, "list_container__AMVNC"))
        )

        html_content = list_container.get_attribute('innerHTML')
        
        # Se pasa el html a la libreria beautifulsoap para analizarlo
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # Extraer la información de los elementos especificados
        nombres = soup.find_all('div', class_='ellipsis_container__ciMmU')
        fechas = soup.find_all('div', class_='athlete-widget_transfer_date__quLhJ')

        
        # Extraer el texto de cada elemento
        nombres_texto = [nombre.get_text(strip=True) for nombre in nombres]
        fechas_texto = [fecha.get_text(strip=True) for fecha in fechas]
        
        # Crear un DataFrame con la información extraída
        data = {
            'Nombre': nombres_texto,
            'Fecha': fechas_texto
        }
        
        df = pd.DataFrame(data)
        
        # Imprimir el DataFrame
        print("DataFrame con la información extraída:")
        return(df)
    except:
        print(f"Error con'{nombre_jugadora}'")
        data = {
        'Nombre': [np.nan],
        'Fecha': [np.nan]
        }
    
        df = pd.DataFrame(data)
        return df
    
def añadir_equipo_fecha_fichaje(df):
    columna = {'Equipo': [], 'Fecha fichaje': []}
    for jugadora in df.index:
        print(df['Nombre'][jugadora])
        informacion = obtener_equipo_fecha_fichaje(df['Nombre'][jugadora])
        columna['Equipo'].append(informacion['Nombre'][0])
        columna['Fecha fichaje'].append(informacion['Fecha'][0])
    nuevo_df = pd.DataFrame(columna)
    df_juntos = pd.concat([df,nuevo_df],axis=1)
    return df_juntos

def get_wnba_stats(link,data_original):
    try:
        options = webdriver.ChromeOptions()
        options.add_argument('--disable-blink-features=AutomationControlled')
        driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)
        
        driver.get(link)
        time.sleep(15)

        nuevo_dataset = {'RANK' : [], 'PLAYER' : [], 'TEAM': [], 'AGE': [], 'GP': [], 'W': [], 'L': [], 'MIN': [], 'PTS': [], 'FGM': [], 'FGA': [], 'FG%': [], '3PM': [], '3PA': [], '3P%': [], 
                  'FTM': [], 'FTA': [], 'FT%': [], 'OREB': [], 'DREB': [], 'REB': [], 'AST': [], 'TOV': [], 'STL': [], 'BLK': [], 'PF': [], 'FP': [], 'DD2': [],'TD3': [], '+/-': []}
        
        
        #headers = ['RANK', 'PLAYER' , 'TEAM', 'AGE', 'GP', 'W', 'L', 'MIN', 'PTS', 'FGM', 'FGA', 'FG%', '3PM', '3PA', '3P%', 
        #          'FTM', 'FTA', 'FT%', 'OREB', 'DREB', 'REB', 'AST', 'TOV', 'STL', 'BLK', 'PF', 'FP', 'DD2','TD3', '+/-']
        
        rows = []

        pages_elements = driver.find_elements(By.CLASS_NAME,"stats-table-pagination__info" )
        number_of_pages = int(pages_elements[0].text[:-2:-1])

        for i in range(number_of_pages):
            if i != 0:
                boton_cambiar = WebDriverWait(driver, 10).until(
                    EC.element_to_be_clickable((By.CSS_SELECTOR, '.stats-table-pagination__next'))
                )
                boton_cambiar.click()
                time.sleep(2)

            player_elements = driver.find_elements(By.CSS_SELECTOR, "table tbody tr:not(.header)")
            
            for row in player_elements:
                cells = row.find_elements(By.TAG_NAME, "td")
                for valor,tipo in zip(cells,nuevo_dataset.keys()):
                    nuevo_dataset[tipo].append(valor.text)
                #if player_data:
                #   rows.append(player_data)
        
        # Crear DataFrame
        nuevo_dataset['RANK'] = nuevo_dataset['RANK'][:len(nuevo_dataset['+/-'])]
        nuevo_dataset['PLAYER'] = nuevo_dataset['PLAYER'][:len(nuevo_dataset['+/-'])]
       
        df = pd.DataFrame(nuevo_dataset)
        
        # Filtrar filas que contienen solo None
        df = df.dropna(how='all')
        df = df.drop('PLAYER', axis = 1)
        df = df.drop('TEAM', axis = 1)
        
        # Filtrar filas que no son jugadoras (como RECORD:, EASTERN CONF:, etc.)
        #df = df[~df['PLAYERS'].str.contains('RECORD:|EASTERN|PPG:|RPG:|APG:|OPG:|2024', regex=True, na=False)]
        
        # Eliminar duplicados
        df = df.drop_duplicates(subset=['RANK'])
        
        # Resetear los índices
        df = df.reset_index(drop=True)
        
        # Guardar en CSV
        csv_path = "indiana_fever_stats.csv"
        df.to_csv(csv_path, index=False)
        print(f"Datos guardados en: {csv_path}")
        
        return df
        
    except Exception as e:
        print(f"Error detallado: {e}")
        if 'driver' in locals():
            driver.save_screenshot("error_detail.png")
        return None
        
    finally:
        if 'driver' in locals():
            driver.quit()


data_jugadoras = obtener_nombre_jugadoras(link_jugadoras_precio[2024])
#for fecha,link in link_jugadoras_precio.items():
#    data_jugadoras = obtener_precio_jugadoras(link,data_jugadoras,fecha)
data_jugadoras = get_wnba_stats(link_jugadoras_stats[2024],data_jugadoras)
print(data_jugadoras)
data_jugadoras.to_csv('datos_prueba.csv', index=False, encoding='utf-8-sig')
'''
data_jugadoras_prueba = data_jugadoras.iloc[0:4]

data_jugadoras_completo = añadir_equipo_fecha_fichaje(data_jugadoras_prueba)
data_jugadoras_completo.to_csv('datos.csv', index=False, encoding='utf-8-sig')
data_jugadoras_completo.info(verbose=True)
'''